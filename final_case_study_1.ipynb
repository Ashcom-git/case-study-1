{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHlcT8AmfLsh",
    "outputId": "74134230-4f43-4581-fdf1-0c4b3082f289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vclGF3x2fYLO"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download pt\n",
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSi4H1fifiM1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import regex as re\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mxotm8ymfuf0"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from datetime import datetime\n",
    "api_token = files.upload()\n",
    "\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
    "\n",
    "!kaggle datasets download -d olistbr/brazilian-ecommerce\n",
    "!unzip '/content/brazilian-ecommerce.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7Z-_EJ3gM2O",
    "outputId": "a80e2ab9-de8a-4e23-857e-4827b50ef958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (80348, 32) (80348,)\n",
      "Test data: (20087, 32) (20087,)\n"
     ]
    }
   ],
   "source": [
    "customers_dataset = pd.read_csv('/content/olist_customers_dataset.csv')\n",
    "geolocation_dataset = pd.read_csv('/content/olist_geolocation_dataset.csv')\n",
    "order_items_dataset = pd.read_csv('/content/olist_order_items_dataset.csv')\n",
    "order_payments_dataset = pd.read_csv('/content/olist_order_payments_dataset.csv')\n",
    "order_reviews_dataset = pd.read_csv('/content/olist_order_reviews_dataset.csv')\n",
    "orders_dataset = pd.read_csv('/content/olist_orders_dataset.csv')\n",
    "products_dataset = pd.read_csv('/content/olist_products_dataset.csv')\n",
    "sellers_dataset = pd.read_csv('/content/olist_sellers_dataset.csv')\n",
    "product_category_name_translation = pd.read_csv('/content/product_category_name_translation.csv')\n",
    "\n",
    "order_items_products = pd.merge(order_items_dataset,products_dataset,on='product_id')\n",
    "order_items_products_sellers = pd.merge(order_items_products,sellers_dataset,on='seller_id')\n",
    "two_order_items_products_sellers = pd.merge(order_items_products_sellers,orders_dataset,on='order_id')\n",
    "two_order_items_products_sellers_customer = pd.merge(two_order_items_products_sellers,customers_dataset,on='customer_id')\n",
    "two_order_items_products_sellers_customer_reviews = pd.merge(two_order_items_products_sellers_customer,order_reviews_dataset,on='order_id')\n",
    "final_dataframe = pd.merge(two_order_items_products_sellers_customer_reviews,order_payments_dataset,on='order_id')\n",
    "\n",
    "mapping = dict(zip(product_category_name_translation['product_category_name'].tolist(),product_category_name_translation['product_category_name_english'].tolist()))\n",
    "final_dataframe['product_category_name'] = final_dataframe['product_category_name'].map(mapping)\n",
    "\n",
    "final_dataframe = final_dataframe.drop_duplicates(subset=['order_id','order_purchase_timestamp','product_id','customer_unique_id','review_comment_message'])\n",
    "final_dataframe.drop(['order_id','product_id','seller_id','customer_unique_id'], axis=1, inplace=True)\n",
    "final_dataframe.dropna(subset=['shipping_limit_date','order_purchase_timestamp','order_delivered_carrier_date','order_delivered_customer_date','order_estimated_delivery_date'], inplace=True)\n",
    "intermediate_time = final_dataframe['order_delivered_customer_date'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").date()) - final_dataframe['order_purchase_timestamp'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").date())\n",
    "final_dataframe['purchase-delivery difference'] = intermediate_time.apply(lambda x:x.days)\n",
    "intermediate_time = final_dataframe['order_estimated_delivery_date'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").date()) - final_dataframe['order_delivered_customer_date'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").date())\n",
    "final_dataframe['estimated-actual delivery difference'] = intermediate_time.apply(lambda x:x.days)\n",
    "\n",
    "final_dataframe['product_category_name'].fillna(value=final_dataframe['product_category_name'].mode()[0], inplace=True)\n",
    "final_dataframe['product_name_lenght'].fillna(value=final_dataframe['product_name_lenght'].mode()[0], inplace=True)\n",
    "final_dataframe['product_description_lenght'].fillna(value=final_dataframe['product_description_lenght'].median(), inplace=True)\n",
    "final_dataframe['product_photos_qty'].fillna(value=final_dataframe['product_photos_qty'].mode()[0], inplace=True)\n",
    "final_dataframe['product_weight_g'].fillna(value=final_dataframe['product_weight_g'].mode()[0], inplace=True)\n",
    "final_dataframe['product_length_cm'].fillna(value=final_dataframe['product_length_cm'].mode()[0], inplace=True)\n",
    "final_dataframe['product_height_cm'].fillna(value=final_dataframe['product_height_cm'].mode()[0], inplace=True)\n",
    "final_dataframe['product_width_cm'].fillna(value=final_dataframe['product_width_cm'].mode()[0], inplace=True)\n",
    "final_dataframe['review_comment_message'].fillna(value='indisponível', inplace=True)\n",
    "\n",
    "final_dataframe['review_score'] = final_dataframe['review_score'].apply(lambda x: 1 if x > 3 else 0)\n",
    "final_dataframe['price_category'] = final_dataframe['price'].apply(lambda x:'expensive' if x>=139 else ('affordable' if x>=40 and x<139 else 'cheap'))\n",
    "final_dataframe = final_dataframe[final_dataframe['order_status'] != 'canceled']\n",
    "final_dataframe['purchase_delivery_diff_per_price'] = final_dataframe['purchase-delivery difference']/final_dataframe['price']\n",
    "final_dataframe.drop(['shipping_limit_date','order_purchase_timestamp','order_approved_at','order_delivered_carrier_date','order_delivered_customer_date','order_estimated_delivery_date','customer_id'], axis=1, inplace=True)\n",
    "\n",
    "labels = final_dataframe['review_score']\n",
    "final_dataframe.drop('review_score', axis=1, inplace=True)\n",
    "final_dataframe['review_availability'] = final_dataframe['review_comment_message'].apply(lambda x: 1 if x != 'indisponível' else 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_dataframe, labels, stratify=labels, test_size=0.2, random_state=0)\n",
    "print('Train data:', X_train.shape, y_train.shape)\n",
    "print('Test data:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MoRBWWENkK2p"
   },
   "outputs": [],
   "source": [
    "#References:\n",
    "#https://stackoverflow.com/a/47218282\n",
    "#https://stackoverflow.com/a/52057778\n",
    "#https://stackoverflow.com/a/11332580\n",
    "#https://stackoverflow.com/a/9532388\n",
    "\n",
    "sp = spacy.load('pt')\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "\n",
    "def process_texts(texts): \n",
    "    processed_text = []\n",
    "    links = '(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)'\n",
    "    dates = '([0-2][0-9]|(3)[0-1])(\\/|\\.)(((0)[0-9])|((1)[0-2]))(\\/|\\.)\\d{2,4}'  \n",
    "    for text in texts:\n",
    "        text = re.sub('[\\n\\r]', ' ', text) \n",
    "        text = re.sub(links, ' URL ', text) \n",
    "        text = re.sub(dates, ' ', text) \n",
    "        text = re.sub('[ \\t]+$', '', text)\n",
    "        text = re.sub('\\W', ' ', text)\n",
    "        text = re.sub('[0-9]+', ' numero ', text)\n",
    "        text = re.sub('\\s+', ' ', text)\n",
    "        text = ' '.join(e for e in text.split() if e.lower() not in all_stopwords) \n",
    "        processed_text.append(text.lower().strip()) \n",
    "    return processed_text\n",
    "\n",
    "def train_response(frame):\n",
    "  f1 = frame[frame.iloc[:,1] == 0]\n",
    "  f2 = frame[frame.iloc[:,1] == 1]\n",
    "  global dict_frame, dict_f1, dict_f2\n",
    "  dict_frame = dict(frame.iloc[:,0].value_counts())\n",
    "  dict_f1 = dict(f1.iloc[:,0].value_counts())\n",
    "  dict_f2 = dict(f2.iloc[:,0].value_counts())\n",
    "  state_0, state_1 = [],[],\n",
    "  for i in range(len(frame)):\n",
    "    if frame.iloc[:,1][i] == 0:\n",
    "      state_0.append(dict_f1.get(frame.iloc[:,0][i],0)/dict_frame[frame.iloc[:,0][i]])\n",
    "      state_1.append(float(1-state_0[-1]))\n",
    "    else:\n",
    "      state_1.append(dict_f2.get(frame.iloc[:,0][i],0)/dict_frame[frame.iloc[:,0][i]])\n",
    "      state_0.append(float(1-state_1[-1])) \n",
    "  df3 = pd.DataFrame({'State_0':state_0, 'State_1':state_1})\n",
    "  return df3.to_numpy()\n",
    "\n",
    "def test_response(test):\n",
    "  t_state_0, t_state_1 = [],[]\n",
    "  for i in range(len(test)):\n",
    "    if dict_frame.get(test[i]):\n",
    "      t_state_0.append(dict_f1.get(test[i],0)/dict_frame.get(test[i]))\n",
    "      t_state_1.append(dict_f2.get(test[i],0)/dict_frame.get(test[i]))\n",
    "    else:\n",
    "      t_state_0.append(0.5)\n",
    "      t_state_1.append(0.5)\n",
    "  df4 = pd.DataFrame({'State_0':t_state_0, 'State_1':t_state_1})\n",
    "  return df4.to_numpy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2-P8l0IpGDb"
   },
   "outputs": [],
   "source": [
    "strn = StandardScaler()\n",
    "strn.fit(X_train[['price','freight_value','product_photos_qty','product_weight_g', 'product_length_cm',\n",
    "       'product_height_cm', 'product_width_cm', 'payment_value','purchase-delivery difference','estimated-actual delivery difference','purchase_delivery_diff_per_price']])\n",
    "X_train_strn = strn.transform(X_train[['price','freight_value','product_photos_qty','product_weight_g', 'product_length_cm',\n",
    "       'product_height_cm', 'product_width_cm', 'payment_value','purchase-delivery difference','estimated-actual delivery difference','purchase_delivery_diff_per_price']])\n",
    "X_test_strn = strn.transform(X_test[['price','freight_value','product_photos_qty','product_weight_g', 'product_length_cm',\n",
    "       'product_height_cm', 'product_width_cm', 'payment_value','purchase-delivery difference','estimated-actual delivery difference','purchase_delivery_diff_per_price']])\n",
    "\n",
    "X_train_resp_prod_cat = train_response(pd.concat([X_train['product_category_name'], y_train], axis=1).reset_index(drop=True))\n",
    "X_test_resp_prod_cat = test_response(X_test['product_category_name'].values)\n",
    "\n",
    "ohe_order_item = OneHotEncoder()\n",
    "ohe_order_item.fit(X_train['order_item_id'].values.reshape(-1,1))\n",
    "X_train_order_item = ohe_order_item.transform(X_train['order_item_id'].values.reshape(-1,1)).toarray()\n",
    "X_test_order_item = ohe_order_item.transform(X_test['order_item_id'].values.reshape(-1,1)).toarray()\n",
    "\n",
    "X_train_resp_payment_seq = train_response(pd.concat([X_train['payment_sequential'], y_train], axis=1).reset_index(drop=True))\n",
    "X_test_resp_payment_seq = test_response(X_test['payment_sequential'].values)\n",
    "\n",
    "ohe_payment_type = OneHotEncoder()\n",
    "ohe_payment_type.fit(X_train['payment_type'].values.reshape(-1,1))\n",
    "X_train_payment_type = ohe_payment_type.transform(X_train['payment_type'].values.reshape(-1,1)).toarray()\n",
    "X_test_payment_type = ohe_payment_type.transform(X_test['payment_type'].values.reshape(-1,1)).toarray()\n",
    "\n",
    "enc_price = OrdinalEncoder()\n",
    "enc_price.fit(X_train['price_category'].values.reshape(-1,1))\n",
    "enc_price.categories_ = [np.array([ 'cheap', 'affordable', 'expensive'], dtype=object)]\n",
    "X_train_cat_price = enc_price.transform(X_train['price_category'].values.reshape(-1,1))\n",
    "X_test_cat_price = enc_price.transform(X_test['price_category'].values.reshape(-1,1))\n",
    "\n",
    "X_train_comment_preprocess = process_texts(X_train['review_comment_message'])\n",
    "X_test_comment_preprocess = process_texts(X_test['review_comment_message'])\n",
    "X_train['embedded_review_comment_message'] = pickle.load(open('/content/drive/MyDrive/Olist/final_models/X_train_embedded_review_comment_message.pkl','rb'))\n",
    "X_test['embedded_review_comment_message'] = pickle.load(open('/content/drive/MyDrive/Olist/final_models/X_test_embedded_review_comment_message.pkl','rb'))\n",
    "\n",
    "\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(X_train_comment_preprocess)\n",
    "X_train_text_input = pad_sequences(tok.texts_to_sequences(X_train_comment_preprocess), padding='post')\n",
    "X_test_text_input = pad_sequences(tok.texts_to_sequences(X_test_comment_preprocess), padding='post')\n",
    "\n",
    "X_train_final = np.concatenate((X_train_strn,X_train_resp_prod_cat,X_train_order_item,\n",
    "       X_train_resp_payment_seq,X_train_payment_type,X_train_cat_price,X_train['review_availability'].values.reshape(-1,1),\n",
    "       np.vstack(X_train['embedded_review_comment_message'].values)), axis=1)\n",
    "\n",
    "X_test_final = np.concatenate((X_test_strn,X_test_resp_prod_cat, X_test_order_item,\n",
    "       X_test_resp_payment_seq,X_test_payment_type,X_test_cat_price,X_test['review_availability'].values.reshape(-1,1),\n",
    "       np.vstack(X_test['embedded_review_comment_message'].values)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGPofEBuwaHB"
   },
   "outputs": [],
   "source": [
    "X_final_truncated = pickle.load(open( \"/content/drive/MyDrive/Olist/final_models/X_final_truncated.pkl\",\"rb\"))\n",
    "X_train_final_truncated = X_final_truncated[:X_train_final.shape[0],:]\n",
    "X_test_final_truncated = X_final_truncated[X_train_final.shape[0]:,:]\n",
    "X_train_final_new, X_cv_final, y_train_new, y_cv = train_test_split(X_train_final_truncated, y_train, stratify=y_train, test_size=0.2, random_state=45)\n",
    "\n",
    "X_train_encode = pickle.load(open('/content/drive/MyDrive/Olist/final_models/X_train_encode.pkl','rb'))\n",
    "X_test_encode = pickle.load(open('/content/drive/MyDrive/Olist/final_models/X_test_encode.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDTPaqMOEneE"
   },
   "outputs": [],
   "source": [
    "def final(x,y,algo,reduction=None):  \n",
    "\n",
    "  if reduction=='hard_svd':\n",
    "    if algo=='knn':\n",
    "      knn_truncate = pickle.load(open('/content/drive/MyDrive/Olist/final_models/knn_clf_truncated.pkl','rb'))\n",
    "      return 'F1 score: {}'.format(f1_score(y,knn_truncate.predict(x),'macro'))\n",
    "    if algo=='log_reg':  \n",
    "      log_reg_truncate = pickle.load(open('/content/drive/MyDrive/Olist/final_models/log_reg_truncated.pkl','rb'))\n",
    "      return 'F1 score: {}'.format(f1_score(y,log_reg_truncate.predict(x),'macro'))\n",
    "    if algo=='rf':\n",
    "      rf_truncate = pickle.load(open('/content/drive/MyDrive/Olist/final_models/rf_truncated.pkl','rb'))\n",
    "      return 'F1 score: {}'.format(f1_score(y,rf_truncate.predict(x),'macro'))\n",
    "    if algo=='xgb':  \n",
    "      xgb_truncate = pickle.load(open('/content/drive/MyDrive/Olist/final_models/xgb_truncated.pkl','rb'))\n",
    "      return 'F1 score: {}'.format(f1_score(y,xgb_truncate.predict(x),'macro'))\n",
    "    if algo=='mlp':\n",
    "      mlp_truncate = load_model('/content/drive/MyDrive/Olist/final_models/mlp_truncated.h5')  \n",
    "      return f1_score(y_test, (mlp_truncate.predict(x)>0.5).astype(int),'macro')\n",
    "\n",
    "  elif reduction=='autoencoders':\n",
    "    if algo=='knn':\n",
    "      knn_encode = pickle.load(open('/content/drive/MyDrive/Olist/final_models/knn_clf_encode.pkl','rb'))\n",
    "      return 'F1 score: {}'.format(f1_score(y,knn_encode.predict(x),'macro'))\n",
    "    if algo=='log_reg':  \n",
    "      log_reg_encode = pickle.load(open('/content/drive/MyDrive/Olist/final_models/log_reg_encode.pkl','rb'))\n",
    "      return 'F1 score: {}'.format(f1_score(y,log_reg_encode.predict(x),'macro'))\n",
    "    if algo=='rf':  \n",
    "      rf_encode = pickle.load(open('/content/drive/MyDrive/Olist/final_models/rf_encode.pkl','rb'))\n",
    "      return 'F1 score: {}'.format(f1_score(y,rf_encode.predict(x),'macro'))\n",
    "    if algo=='xgb':  \n",
    "      xgb_encode = pickle.load(open('/content/drive/MyDrive/Olist/final_models/xgb_encode.pkl','rb'))\n",
    "      return 'F1 score: {}'.format(f1_score(y,xgb_encode.predict(x),'macro'))\n",
    "    if algo=='mlp':\n",
    "        mlp_encode = load_model('/content/drive/MyDrive/Olist/final_models/mlp_encode.h5')  \n",
    "        return 'F1 score: {}'.format(f1_score(y_test, (mlp_encode.predict(x)>0.5).astype(int),'macro')) \n",
    "\n",
    "  else:\n",
    "    if algo=='rnn':\n",
    "      rnn_data = load_model('/content/drive/MyDrive/Olist/final_models/model_rnn.h5')   \n",
    "      return 'F1 score: {}'.format(f1_score(y_test, (rnn_data.predict([x[0],x[1][:,:-300]])>0.5).astype(int),'macro'))\n",
    "    if algo=='cnn_rnn':  \n",
    "      cnn_rnn_data = load_model('/content/drive/MyDrive/Olist/final_models/model_cnn_rnn.h5')\n",
    "      return 'F1 score: {}'.format(f1_score(y_test, (cnn_rnn_data.predict([x[0],x[1][:,:-300]])>0.5).astype(int),'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "_fFHO94YxTYX",
    "outputId": "b7ca9c3a-471a-4637-ec00-63e47f71b3d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 218 ms, sys: 63.5 ms, total: 282 ms\n",
      "Wall time: 1.23 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'F1 score: 0.8738261430213325'"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "final(X_test_final_truncated,y_test,'xgb','autoencoders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8Pmja9vSRgH",
    "outputId": "9b8ce976-2b2c-4342-d2e2-8f3320d0eb28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 884 ms, sys: 89 ms, total: 973 ms\n",
      "Wall time: 1.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9091929649494824"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "final(X_test_final_truncated,y_test,'mlp','hard_svd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "JD2e_3GpxghY",
    "outputId": "bc1ac28d-d1bf-4320-e455-8e243748caf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 7.32 s, total: 17.6 s\n",
      "Wall time: 1min 16s\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'F1 score: 0.9182254491754861'"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "final([X_test_text_input,X_test_final],y_test,'rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "0IzGVT33Qwok",
    "outputId": "dc32f20f-d158-4686-cc49-2519120a5006"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.75 s, sys: 1.6 s, total: 7.35 s\n",
      "Wall time: 33.9 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'F1 score: 0.902708001470408'"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "final([X_test_text_input,X_test_final],y_test,'cnn_rnn')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "final_case_study_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
